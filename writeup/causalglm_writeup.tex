\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{cmbright}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pxfonts}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}

\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
{hyperref}
\hypersetup{
pdfstartview={XYZ null null 1}}
\usepackage{breakurl}



\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\expit}{expit}

\usepackage{natbib}
\bibliographystyle{apalike}


\title{%
  causalglm \\
  \large Semiparametric and nonparametric generalized linear models for conditional causal inference using Targeted Maximum Likelihood Estimation
  }
\author{Lars van der Laan }
\date{September 2021}

 
\usepackage{fancyhdr}          % this and next line are for fancy headers/footers
\pagestyle{fancy}



\usepackage{Sweave}
\begin{document}
\input{causalglm_writeup-concordance}

\maketitle



\section{Introduction to causalglm}
\subsection{Semiparametric and nonparametric generalized linear models for conditional causal inference using Targeted Maximum Likelihood Estimation}


It is possible to get robust and efficient inference for causal quantities using machine-learning. In the search for answers to causal questions, assuming parametric models can be dangerous. With even a seemingly small amount of confounding and misspecificaton, they can give biased answers. One way of mitigating this challenge is to instead assume a parametric model for only the feature of the data-generating distribution that you care about. That is, assume a semiparametric model! Let the data speak for itself and use machine-learning to model the nuisance features of the data that are not directly related to your causal question. Why worry about things that don't matter for your question? It is not worth the risk of being wrong.

In this package, we utilize targeted machine-learning to generalize the parametric generalized linear models commonly used for treatment effect estimation (e.g. the R package glm) to the world of semi and nonparametric models. There is little-to-no loss in precision/p-values/confidence-interval-widths with these semiparametric methods relative to parametric generalized linear models, but the bias reduction from these methods can be substantial! Simulations suggest that these methods can work well with small sample sizes. We employ ensemble machine-learning (Super-Learning) that adapts the aggressiveness of the ML algorithms with sample size, thereby allowing for robust and correct inference in a diverse range of settings. All methods utilize targeted maximum likelihood estimation (TMLE) (van der Laan, Rose, 2011).\nocite{vanderLaanRose2011}

Each estimand considered in this package can be modeled with a user-specified parametric model that is either assumed correct (`spglm` and `causalglmnet`) or as an approximation, i.e. working model, of the nonparametric true estimand (`npglm`). The former approach provides interpretable estimates and correct inference only when the parametric model is correct, and the latter approach provides interpretable estimates and nonparametrically correct inference even when the parametric model is incorrect.

By default, the Highly Adaptive Lasso (HAL) and its semiparametric variants are used for estimation. See Benkeser et al. (2016) for an overview of HAL and its statistical performance. For the theoretical analysis of HAL including its fast rate of convergence in high dimensions, see Bibaut et al. (2019) and van der Laan (2017) \nocite{bibaut2019fast}\nocite{HAL2016}\nocite{vanderlaanGenerlaTMLE}. We implement the HAL estimators using the R package hal9001 (Coyle et al., 2021).\nocite{hal1}\nocite{hal2} 
\nocite{vanderlaanGenerlaTMLE} 

Noticable features supported:
\begin{itemize}
\item Efficent semiparametric and nonparametric inference for user-specified parametric working-models of conditional treatment-effect functions with `spglm` and `npglm`.
\item  Efficient nonparametric inference for marginal structural models for the CATE, CATT, TSM and RR with `npglm`.
\item  General machine-learning tools with the tlverse/sl3 generalized machine-learning ecosystem (Coyle et al., 2021).\nocite{coyle2021sl3}
\item  High dimensional covariates and variable selection for confounders with the wrapper function `causalglmnet`.
\item  Interpretable semiparametric and nonparametric estimates and efficient inference even with adaptive estimation and variable selection.
\item  Designed-easy-to-use interface and built-in machine-learning routines for diverse settings and immediate use.
\end{itemize}

\subsection{spglm: semiparametric causal inference for generalized linear models}

This function supports semiparametric efficient estimation of following point-treatment estimands:

\begin{itemize}
\item Conditional average treatment effect (CATE). (Causal semiparametric linear regression)
\item Conditional odds ratio (OR) between two binary variables. (Causal semiparametric logistic regression)
\item Conditional relative risk (RR) for nonnegative outcomes and a binary treatment. (Causal semiparametric log-linear relative-risk regression)

\end{itemize}



\subsection{npglm: nonparametric causal inference for generalized linear models}

The function npglm supports nonparametric efficient working-model-based estimation of following point-treatment estimands:

\begin{itemize}
\item Conditional average treatment effect (CATE).
\item Conditional average treatment effect among the treated (CATT)
\item Conditional treatment-specific mean (TSM)
\item Conditional odds ratio (OR) between two binary variables. 
\item Conditional relative risk (RR) for nonnegative outcomes and a binary treatment.  
\end{itemize}

This function also automatically supports marginal structural models by specifying lower dimensional formulas/working-models for the following estimands
\begin{itemize}
\item Marginal structural models for the conditional average treatment effect (CATE).
\item Marginal structural models for the conditional average treatment effect among the treated (CATT).
\item Marginal structural models for the conditional treatment-specific mean (TSM).
\item Marginal structural models for the conditional relative risk (RR). 
\end{itemize}


\subsection{causalglmnet: high dimensional causal inference for generalized linear models}
The function causalglmnet supports causal inference with high dimensional confounders for the following estimands:
\begin{itemize}
\item Conditional average treatment effect (CATE). 
\item Conditional odds ratio (OR) between two binary variables. 
\item Conditional relative risk (RR) for nonnegative outcomes and a binary treatment.  
\end{itemize}
Note that the function causalglmnet is a custom wrapper function around the function spglm.

\subsection{npcoxph: assumption-lean inference for the conditional hazard ratio}
The function npcoxph supports the following survival estimands:
\begin{itemize}

\item Nonparametric inference for a user-specified working-model for the conditional hazard ratio between two treatments with `npcoxph`.
\item The estimands supported by `npcoxph` based on lower dimensional formulas can immediately be interpreted as marginal structural models for the hazard ratio.
\end{itemize}





\section{Data-Structure and treatment-effect estimands}

We will mainly consider the point-treatment data-structure $O = (W,A,Y)$ where $W$ represents a vector of baseline covariates (i.e. possible confounders), $A$ is a binary treatment assignment, and $Y$ is some outcome variable. As an example, for a given observation $O$, $W$ could be measurements: age, sex, a risk-score, location, income; $A$ could take the value $1$ if the individual receives the treatment and $0$ if they do not receive the treatment; and $Y$ is a binary or continuous variable that captures the effect of the treatment. For the goal of assessing heterogeneity of the treatment effect, there are a number of popular estimands:

\noindent The conditional average treatment effect (CATE):
\begin{equation}
CATE(w) := E[Y|A=1,W=w] - E[Y|A=0, W=w],
\end{equation}
which is an additive measure of the effect of the treatment ($A=1$) relative to no treatment $(A=0)$.

\vspace{0.5cm}


\noindent The conditional odds ratio (OR) for when $Y$ is binary:
\begin{equation}
OR(w) := \frac{P(Y=1|A=1,W=w)/P(Y=0|A=1,W=w)}{P(Y=1|A=0,W=w)/P(Y=0|A=0,W=w)}
\end{equation}

\vspace{0.5cm}

\noindent The conditional relative risk (RR) for when $Y$ is nonnegative (e.g. a binary or count variable):
\begin{equation}
RR(w) := \frac{E[Y|A=1,W=w]}{E[Y|A=0,W=w]},
\end{equation}
which is a relative measure of the effect of the treatment ($A=1$) relative to no treatment $(A=0)$.

\vspace{0.5cm}

\noindent In some application, non-contrast measures like the conditional treatment-specific mean (TSM) may be of interest:
\begin{equation}
TSM_a(w) := E[Y|A=a,W=w].
\end{equation}

\subsection{Conventional estimators using parametric generalized linear models}
In order to estimate the estimands of the previous section, parametric generalized linear models are often employed (e.g. the R package \textit{glm}). For the CATE,the following linear regression model is often used:
$$E[Y|A, W=w] = \beta_0 A +  \beta_1^T w \cdot A + \beta_2^T w.$$
This model is equivalent to assuming both the nuisance linear model
$$E[Y|A=0,W=w] =  \beta_2^T w$$
and target linear model
$$CATE(w) = \beta_0 + \beta_1^T w.$$
Thus, the coefficient in front of the treatment interactions can be directly interpreted as a measure of the conditional average treatment effect when the parametric model is correct. However, we see that very strong parametric assumptions are made on the orthogonal nuisance function $w \mapsto E[Y|A=0,W=w]$, which has little to do with the CATE.

Next, for the conditional odds ratio, the following logistic regression model is often used
$$\logit \left\{P(Y=1|A,W=w) \right\} =  \beta_0 A +  \beta_1^T w \cdot A + \beta_2^T w.$$
This model is equivalent to assuming both the nuisance logistic model
$$\logit\left\{P(Y=0|A=0,W=w) \right\} =   \beta_2^T w$$
and the target logistic model
$$\log OR(w) =  \beta_0  +  \beta_1^T w. $$
Once again, we see that the conventional logistic regression model makes strong parametric assumptions on the orthogonal nuisance parameter $P(Y=0|A=0,W=w).$ 

Finally, for the conditional relative-risk, the poisson or log-linear regression model is a well-known approach:
$$\log \left\{E[Y|A,W=w] \right\} =  \beta_0 A +  \beta_1^T w \cdot A + \beta_2^T w.$$
This is equivalent to assuming the nuisance log-linear model
$$\log \left\{E[Y|A=0,W=w] \right\} = \beta_2^T w$$
and the target log-linear model
$$\log \left\{RR(w) \right\} =  \beta_0  +  \beta_1^T w. $$
Besides the strong parametric assumptions on the nuisance parameter $E[Y|A=0,W=w]$, another issue with this approach is that conventional methods only provide inference when the outcome is Poisson distributed, which is not useful if the outcome is binary. Log-link binomial generalized-linear-models are one way to overcome this limitation. 

Under the parametric assumptions, standard generalize linear model software can be used to obtain estimates and inference for the coefficients in the above models for the treatment-effect estimands. However, these methods make much stronger assumptions than are needed. In particular, the parametric assumptions on $E[Y|A=0,W]$ provides little-to-no benefit in interpretability since we are interested in the coefficients for the treatment interaction terms, and it comes at a possibly substantial cost in bias due to model misspecification. Additionally, these methods do not allow for any adaptive estimation of $E[Y|A=0,W]$ (e.g. using the LASSO, variable selection, or machine-learning) and therefore do not perform well in both estimation and inference in high dimensions. In the next section, we consider a partial relaxation of the parametric models through so-called partially-linear generalized linear models.




\section{Semiparametric generalized linear models for treatment effect estimation  with spglm}

In this section, we give an overview of semiparametric treatment-effect estimation in partially-linear generalized linear models which allows for adaptive estimation of nuisance parameters of the data-generating distribution that are not directly relevant for the problem at end. Semiparametric models are statistical models in which one component of the data-generating distribution is parametric and the remaining components are nonparametric. For background on semiparametric models and estimators in causal inference, we refer to Bickel et al. (1993) and van der Laan, Robins (2003).\nocite{vanderlaanunified}\nocite{bickel1993efficient}



These methods allow for:
\begin{enumerate}
\item Interpretable (coefficient-based) inference for user-specified parametric models for conditional treatment effect estimands.
\item Adaptive machine-learning and variable selection methods including generalized additive models, LASSO, MARS and gradient-boosting can be used to estimate nuisance parameters nonparametrically, thereby substantially relaxing assumptions for valid inference although still assuming a parametric model for the conditional estimand of interest.
\end{enumerate}

\subsection{Conditional average treatment effect (CATE) and partially-linear least-squares regression}
Let $\underline{f}(w)$ be an arbitrary known vector-valued function of the covariates and consider the linear parametric model $\beta^T \underline{f}(w)$ for $CATE(w)$. The partially-linear least-squares model is of the form:
$$E[Y|A,W=w] = \beta^T \underline{f}(w) \cdot A + h_0(w),$$
where $ h_0(w) := E[Y|A=0,W=w]$ is an unspecified nuisance function that is to be learned from the data nonparametrically. The parametric component of the model is the coefficient vector $\beta$. This model is equivalent to \textit{only} assuming:
$$CATE(w) = \beta^T \underline{f}(w).$$
Thus, this semiparametric model only makes assumptions that directly relate the estimand of interest! 

\noindent A concrete model is choosing $\underline{f}(W) = (1,W)$ which gives the linear model
$$CATE(w) = \beta_0 + \beta_1^T w.$$

Estimates and inference for the coefficient vector in this semiparametric model can be obtained by applying the R function \textit{spglm} with the option `estimand = "CATE"`. We employ machine-learning for initial estimation of the relevant components of the data-generating distribution and then use targeted maximum likelihood estimation for bias-correction, thereby allowing for valid efficient inference. The estimand is estimated using targeted maximum likelihood estimation and the theory and pseudo-code for the method can be found in the working paper, van der Laan (2009). \nocite{OddsRatioreadingsTMLE}

\subsection{Conditional odds ratio (OR) and partially-linear logistic regression}
Let $\underline{f}(w)$ be an arbitrary known vector-valued function of the covariates and consider the parametric model $\beta^T \underline{f}(w)$ for $\log OR(w)$. The partially-linear logistic model is given by:
$$\logit\left\{E[Y|A,W=w]\right\} = \beta^T \underline{f}(w) \cdot A + h_0(w),$$
where $ h_0(w) := \logit\left\{E[Y|A=0,W=w]\right\}$ is an unspecified nuisance function that is to be learned from the data nonparametrically. This model is equivalent to \textit{only} assuming:
$$\log OR(w) = \beta^T \underline{f}(w).$$

Estimates and inference for the coefficient vector in this semiparametric model can be obtained by applying the R function \textit{spglm} with the option `estimand = "OR"`. The estimand is estimated using targeted maximum likelihood estimation and the theory and pseudo-code for the method can be found in the working paper, van der Laan (2009). \nocite{OddsRatioreadingsTMLE}
For a similar estimator based on estimating equations and additional background on the semiparametric logistic regression model, see Tchetgen Tchetgen (2010). \nocite{TchetgenOddsRatio}



\subsection{Conditional relative-risk (RR) and partially-linear log-linear regression}
Let $\underline{f}(w)$ be an arbitrary known vector-valued function of the covariates and consider the parametric model $\beta^T \underline{f}(w)$ for $\log RR(w)$. The partially-linear log-linear model is of the form:
$$\log\left\{E[Y|A,W=w]\right\} = \beta^T \underline{f}(w) \cdot A + h_0(w),$$
where $ h_0(w) := \log\left\{E[Y|A=0,W=w]\right\}$ is an unspecified nuisance function that is to be learned from the data nonparametrically. This model is equivalent to \textit{only} assuming:
$$\log RR(w) =\beta^T \underline{f}(w).$$


Estimates and inference for the coefficient vector in this semiparametric model can be obtained by applying the R function \textit{spglm} with the option `estimand = "RR"`. The estimand is estimated using targeted maximum likelihood estimation and the theory and pseudo-code for the method can be found in the working paper, Tuglus et al. (2011).\nocite{TMLERR} 

\subsection{spglm in practice}

Let us generate a mock dataset that has a constant CATE of value 1.
\begin{Schunk}
\begin{Sinput}
> library(causalglm)
> n <- 250
> W <- runif(n, min = -1, max = 1)
> A <- rbinom(n, size = 1, prob = plogis(W))
> Y <- rnorm(n, mean = A + W, sd = 0.3)
> data <- data.frame(W, A, Y)
\end{Sinput}
\end{Schunk}

We specify the parametric form of the CATE through the formula argument. In this case, we will use the intercept-only formula which is equivalent to assuming the CATE is constant.  The output consists of:
a coefficient estimate for the intercept, lower and upper confidence intervals, an asymptotic standard error estimate for the estimator, a Z-score and p-value. The argument `W` should be a character vector of variable names in data for which to adjust, `A` should be the name of a treatment variable, and `Y` should be the name of an outcome variable. We set the argument `estimand = "CATE"` to estimate the conditional average treatment effect.
\begin{Schunk}
\begin{Sinput}
> formula <- ~1
> output <-
+   spglm(
+     formula,
+     data,
+     W = "W", A = "A", Y = "Y",
+     estimand = "CATE",
+     verbose = FALSE
+   )
> summary(output)
\end{Sinput}
\begin{Soutput}
   type       param tmle_est         se     lower    upper  Z_score p_value
1: CATE (Intercept) 1.006318 0.04279064 0.9224498 1.090186 371.8402       0
\end{Soutput}
\end{Schunk}
We can also do a much more complex model with higher dimensional treatment effect interactions. The following data distribution has $CATE(w) = 1 + w$.  
\begin{Schunk}
\begin{Sinput}
> Y <-
+   rnorm(n,
+     mean = A * W + A + poly(W, degree = 3) + sin(4 * W),
+     sd = 0.4
+   )
> data <- data.frame(W, A, Y)
> formula <- ~ 1 + W
> output <-
+   spglm(
+     formula,
+     data,
+     W = "W", A = "A", Y = "Y",
+     estimand = "CATE",
+     verbose = FALSE
+   )
> summary(output)
\end{Sinput}
\begin{Soutput}
   type       param  tmle_est         se     lower    upper  Z_score p_value
1: CATE (Intercept) 0.9722941 0.04648750 0.8811803 1.063408 330.6979       0
2: CATE           W 0.8611382 0.07953334 0.7052557 1.017021 171.1960       0
\end{Soutput}
\end{Schunk}


Currently, the nonparametric learning is performed using the partially-linear first-order Highly Adaptive Lasso (HAL) implemented using the R package tlverse/hal9001. HAL is an adaptive piece-wise linear regression spline estimator and the custom implementation performs the risk minimization entirely within the semiparametric model. It is implemented using LASSO regression with the parametric treatment interactions (as specified by the formula argument) unpenalized and a rich penalized spline basis for the nonparametric component of the model. This method performs risk minimization entirely within the semiparametric model. There are a number of other built-in learning options: c("HAL", "SuperLearner", "glm", "glmnet", "gam", "mars", "ranger", "xgboost")

\begin{Schunk}
\begin{Sinput}
> Y <- rnorm(n, mean = A * W + A + W, sd = 0.4)
> data <- data.frame(W, A, Y)
> # generalized additive models:
> output <-
+   spglm(
+     formula,
+     data,
+     W = "W", A = "A", Y = "Y",
+     estimand = "CATE",
+     learning_method = "gam",
+     verbose = FALSE
+   )
> summary(output)
\end{Sinput}
\begin{Soutput}
   type       param tmle_est         se     lower    upper  Z_score p_value
1: CATE (Intercept) 0.993086 0.05340679 0.8884106 1.097761 294.0088       0
2: CATE           W 1.099507 0.09354731 0.9161576 1.282856 185.8389       0
\end{Soutput}
\begin{Sinput}
> # multivariate adaptive regression splines:
> output <-
+   spglm(
+     formula,
+     data,
+     W = "W", A = "A", Y = "Y",
+     estimand = "CATE",
+     learning_method = "mars",
+     verbose = FALSE
+   )
> summary(output)
\end{Sinput}
\begin{Soutput}
   type       param  tmle_est         se     lower    upper  Z_score p_value
1: CATE (Intercept) 0.9957452 0.05361759 0.8906566 1.100834 293.6371       0
2: CATE           W 1.1226227 0.09426294 0.9378708 1.307375 188.3054       0
\end{Soutput}
\begin{Sinput}
> # gradient-boosting with xgboost: :
> output <-
+   spglm(
+     formula,
+     data,
+     W = "W", A = "A", Y = "Y",
+     estimand = "CATE",
+     learning_method = "xgboost",
+     verbose = FALSE
+   )
> summary(output)
\end{Sinput}
\begin{Soutput}
   type       param  tmle_est         se     lower    upper  Z_score p_value
1: CATE (Intercept) 0.9926526 0.05688734 0.8811554 1.104150 275.9000       0
2: CATE           W 1.1174258 0.10326843 0.9150234 1.319828 171.0886       0
\end{Soutput}
\end{Schunk}


The default learning algorithm "HAL" can be customized with the HAL\_args\_Y0W argument (see the arguments in hal9001 for more description). `max\_degree` = 1 corresponds with estimating the nuisance function $E[Y|A=0,W]$ with an additive model. `num\_knots` specifies for each interaction degree how many variable knot points are used to generate the tensor product interaction basis functions. 

\begin{Schunk}
\begin{Sinput}
> HAL_args_Y0W <-
+   list(
+     smoothness_orders = 1,
+     max_degree = 2,
+     num_knots = c(10, 5, 1)
+   )
> output <-
+   spglm(
+     formula,
+     data,
+     W = "W", A = "A", Y = "Y",
+     estimand = "CATE",
+     learning_method = "HAL",
+     HAL_args_Y0W = HAL_args_Y0W,
+     verbose = FALSE
+   )